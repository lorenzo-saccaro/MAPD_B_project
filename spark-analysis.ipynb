{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef46f7d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ADMIN SECTION: create and delete topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b0fb15",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS = '10.67.22.61:9092'\n",
    "TOPIC_NAME = 'results'\n",
    "N_PARTITIONS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86c7a3e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from  confluent_kafka.admin import AdminClient, NewTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9679ec1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kafka_admin = AdminClient({'bootstrap.servers':KAFKA_BOOTSTRAP_SERVERS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc387bc2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_topics(admin, topics):\n",
    "    \"\"\" Create topics \"\"\"\n",
    "\n",
    "    new_topics = [NewTopic(topic, num_partitions=N_PARTITIONS, replication_factor=1) for topic in topics]\n",
    "    # Call create_topics to asynchronously create topics, a dict\n",
    "    # of <topic,future> is returned.\n",
    "    fs = admin.create_topics(new_topics, request_timeout=15.0)\n",
    "\n",
    "    # Wait for operation to finish.\n",
    "    # Timeouts are preferably controlled by passing request_timeout=15.0\n",
    "    # to the create_topics() call.\n",
    "    # All futures will finish at the same time.\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()  # The result itself is None\n",
    "            print(\"Topic {} created\".format(topic))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create topic {}: {}\".format(topic, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c375e97",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def delete_topics(admin, topics):\n",
    "    \"\"\" delete topics \"\"\"\n",
    "\n",
    "    # Call delete_topics to asynchronously delete topics, a future is returned.\n",
    "    # By default, this operation on the broker returns immediately while\n",
    "    # topics are deleted in the background. But here we give it some time (30s)\n",
    "    # to propagate in the cluster before returning.\n",
    "    #\n",
    "    # Returns a dict of <topic,future>.\n",
    "    fs = admin.delete_topics(topics, operation_timeout=30)\n",
    "\n",
    "    # Wait for operation to finish.\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()  # The result itself is None\n",
    "            print(\"Topic {} deleted\".format(topic))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to delete topic {}: {}\".format(topic, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc137777",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic results deleted\n"
     ]
    }
   ],
   "source": [
    "delete_topics(kafka_admin, [TOPIC_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e6099c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create topic results: KafkaError{code=TOPIC_ALREADY_EXISTS,val=36,str=\"Topic 'results' is marked for deletion.\"}\n"
     ]
    }
   ],
   "source": [
    "## check if topic already exits otherwise create it\n",
    "if not TOPIC_NAME in kafka_admin.list_topics().topics.keys():\n",
    "    create_topics(kafka_admin, [TOPIC_NAME])\n",
    "else:\n",
    "    print(\"Topic \" + TOPIC_NAME + \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b072fd9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746d059e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb20feb3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "producer = Producer({'bootstrap.servers':KAFKA_BOOTSTRAP_SERVERS,\n",
    "                     'linger.ms':0, # delay in ms before messages are sent if batch size is not reached\n",
    "                     'batch.size':16384}) # maximum batch size before messages are sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca15e4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SPARK STRUCTURED STREAMING ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2cc467",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, countDistinct, count\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e5b20fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://10.67.22.29:7077\")\\\n",
    "        .appName(\"Test streaming\")\\\n",
    "        .config('spark.executor.memory', '4g')\\\n",
    "        .config('spark.driver.memory', '1500m')\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\")\\\n",
    "        .config(\"spark.eventLog.enabled\", 'true')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "        # .config('spark.executor.instances', '4')\\\n",
    "        # .config('spark.executor.cores', '1')\\\n",
    "        # .config('spark.executor.memory', '1g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d983274",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# this may be the only option that matters (set it equal to the number of cores)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2d113ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# not working with streaming df\n",
    "# spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.cbo.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46fee9d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.67.22.29:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://10.67.22.29:7077 appName=Test streaming>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5eb99f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read streaming df from kafka\n",
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 4000)\\\n",
    "    .option('subscribe', 'data')\\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db325de7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d625d6a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "476e568c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extract the value from the kafka message\n",
    "csv_df = inputDF.select(col(\"value\").cast(\"string\")).alias(\"csv\").select(\"csv.*\")\n",
    "\n",
    "# split the csv line in the corresponding fields\n",
    "df = csv_df.selectExpr(\"cast(split(value, ',')[0] as int) as HEAD\",\n",
    "                       \"cast(split(value, ',')[1] as int) as FPGA\",\n",
    "                       \"cast(split(value, ',')[2] as int) as TDC_CHANNEL\",\n",
    "                       \"cast(split(value, ',')[3] as long) as ORBIT_CNT\",\n",
    "                       \"cast(split(value, ',')[4] as int) as BX_COUNTER\",\n",
    "                       \"cast(split(value, ',')[5] as double) as TDC_MEAS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c99972d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: long (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edaaeaf8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove unwanted rows\n",
    "df = df.filter(df.HEAD==2)\n",
    "\n",
    "# add CHAMBER column for easier grouping later\n",
    "df = df.withColumn(\"CHAMBER\", \\\n",
    "                    when((df.FPGA == 0)&(df.TDC_CHANNEL>=0)&(df.TDC_CHANNEL<64), 0) \\\n",
    "                   .when((df.FPGA == 0)&(df.TDC_CHANNEL>=64)&(df.TDC_CHANNEL<128), 1 ) \\\n",
    "                   .when((df.FPGA == 1)&(df.TDC_CHANNEL>=0)&(df.TDC_CHANNEL<64), 2 ) \\\n",
    "                   .when((df.FPGA == 1)&(df.TDC_CHANNEL>=64)&(df.TDC_CHANNEL<128), 3 )\n",
    "                  )\n",
    "\n",
    "# compute absolute time\n",
    "df = df.withColumn(\"ABSOLUTE_TIME\", 25*(df.ORBIT_CNT * 3564 + df.BX_COUNTER + df.TDC_MEAS/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a3ecc05",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: long (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      " |-- CHAMBER: integer (nullable = true)\n",
      " |-- ABSOLUTE_TIME: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbb9eef8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "time_offset_by_chamber = {0: 95.0 - 1.1, # Ch 0\n",
    "                          1: 95.0 + 6.4, # Ch 1\n",
    "                          2: 95.0 + 0.5, # Ch 2\n",
    "                          3: 95.0 - 2.6, # Ch 3\n",
    "                         }\n",
    "\n",
    "edges_list = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,np.inf]\n",
    "edges_list_to_print = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75]\n",
    "    \n",
    "edge_list_b = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,np.inf]\n",
    "edge_list_b_to_print = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "    \n",
    "edge_list_c = [0,30,60,90,120,150,180,210,240,270,300,330,360,390,420,450,480,510,np.inf]\n",
    "edge_list_c_to_print = [0,30,60,90,120,150,180,210,240,270,300,330,360,390,420,450,480,510,540]\n",
    "\n",
    "ID = -1\n",
    "\n",
    "# function to apply to each batch: writes and send a kafka message at the end\n",
    "def batch_func(df, epoch_id):\n",
    "\n",
    "    df = df.persist()\n",
    "\n",
    "    # 1: total number of processed hits, post-cleansing (1 value per batch)\n",
    "\n",
    "    hit_count = df.count()\n",
    "\n",
    "    # 2: total number of processed hits, post-cleansing, per chamber (4 values per batch)\n",
    "\n",
    "    hit_count_chamber = df.groupby('CHAMBER').agg(count('TDC_CHANNEL').alias('HIT_COUNT')).sort(\"CHAMBER\").select('HIT_COUNT')\n",
    "\n",
    "    # 3: histogram of the counts of active TDC_CHANNEL, per chamber (4 arrays per batch)\n",
    "\n",
    "    tdc_counts = df.groupby(['CHAMBER', 'TDC_CHANNEL']).agg(count('ORBIT_CNT').alias('TDC_COUNTS'))\n",
    "    tdc_counts = tdc_counts.persist()\n",
    "\n",
    "    ch0_tdc_counts = tdc_counts.filter(tdc_counts.CHAMBER==0).select('TDC_COUNTS')\n",
    "    ch1_tdc_counts = tdc_counts.filter(tdc_counts.CHAMBER==1).select('TDC_COUNTS')\n",
    "    ch2_tdc_counts = tdc_counts.filter(tdc_counts.CHAMBER==2).select('TDC_COUNTS')\n",
    "    ch3_tdc_counts = tdc_counts.filter(tdc_counts.CHAMBER==3).select('TDC_COUNTS')\n",
    "\n",
    "    # same query slightly slower\n",
    "    # ch0_tdc_counts = df.filter(df.CHAMBER==0).groupby('TDC_CHANNEL').agg(count('ORBIT_CNT').alias('TDC_COUNTS')).select('TDC_COUNTS')\n",
    "    # ch1_tdc_counts = df.filter(df.CHAMBER==1).groupby('TDC_CHANNEL').agg(count('ORBIT_CNT').alias('TDC_COUNTS')).select('TDC_COUNTS')\n",
    "    # ch2_tdc_counts = df.filter(df.CHAMBER==2).groupby('TDC_CHANNEL').agg(count('ORBIT_CNT').alias('TDC_COUNTS')).select('TDC_COUNTS')\n",
    "    # ch3_tdc_counts = df.filter(df.CHAMBER==3).groupby('TDC_CHANNEL').agg(count('ORBIT_CNT').alias('TDC_COUNTS')).select('TDC_COUNTS')\n",
    "\n",
    "    # 4: histogram of the total number of active TDC_CHANNEL in each ORBIT_CNT, per chamber (4 arrays per batch)\n",
    "\n",
    "    tdc_active = df.groupby(['CHAMBER', 'ORBIT_CNT']).agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\"))\n",
    "    tdc_active = tdc_active.persist()\n",
    "\n",
    "    ch0_tdc_active = tdc_active.filter(tdc_active.CHAMBER==0).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch1_tdc_active = tdc_active.filter(tdc_active.CHAMBER==1).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch2_tdc_active = tdc_active.filter(tdc_active.CHAMBER==2).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch3_tdc_active = tdc_active.filter(tdc_active.CHAMBER==3).select('ACTIVE_TDC_CHANNEL')\n",
    "\n",
    "    # same query slightly slower\n",
    "    # ch0_tdc_active = df.filter(df.CHAMBER==0).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch1_tdc_active = df.filter(df.CHAMBER==1).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch2_tdc_active = df.filter(df.CHAMBER==2).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch3_tdc_active = df.filter(df.CHAMBER==3).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "\n",
    "    # optional: drift time computation\n",
    "\n",
    "    # get scintillator signals\n",
    "    t0 = df.filter((df.FPGA==1) & (df.TDC_CHANNEL==128)).selectExpr('ORBIT_CNT as ORBIT_CNT_T0', 'ABSOLUTE_TIME as ABSOLUTE_TIME_T0')\n",
    "    # TODO: add check for multiple signal in same orbit\n",
    "\n",
    "    # get only events with a signal from the scintillator and compute drift time\n",
    "    df_scint = df.join(t0, df.ORBIT_CNT==t0.ORBIT_CNT_T0, 'inner').selectExpr('CHAMBER', 'TDC_CHANNEL', 'ORBIT_CNT', 'ABSOLUTE_TIME-ABSOLUTE_TIME_T0 as DRIFT_TIME')\n",
    "\n",
    "    # adjust drift time with the correct\n",
    "    df_scint = df_scint.withColumn('DRIFT_TIME',\n",
    "                       when(df_scint.CHAMBER==0, df_scint.DRIFT_TIME+time_offset_by_chamber[0]) \\\n",
    "                      .when(df_scint.CHAMBER==1, df_scint.DRIFT_TIME+time_offset_by_chamber[1]) \\\n",
    "                      .when(df_scint.CHAMBER==2, df_scint.DRIFT_TIME+time_offset_by_chamber[2]) \\\n",
    "                      .when(df_scint.CHAMBER==3, df_scint.DRIFT_TIME+time_offset_by_chamber[3])\n",
    "                  )\n",
    "\n",
    "    df_scint = df_scint.persist()\n",
    "\n",
    "    tdc_active_scint = df_scint.groupby(['CHAMBER', 'ORBIT_CNT']).agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\"))\n",
    "    tdc_active_scint = tdc_active_scint.persist()\n",
    "\n",
    "    ch0_tdc_active_scint = tdc_active_scint.filter(tdc_active_scint.CHAMBER==0).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch1_tdc_active_scint = tdc_active_scint.filter(tdc_active_scint.CHAMBER==1).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch2_tdc_active_scint = tdc_active_scint.filter(tdc_active_scint.CHAMBER==2).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch3_tdc_active_scint = tdc_active_scint.filter(tdc_active_scint.CHAMBER==3).select('ACTIVE_TDC_CHANNEL')\n",
    "\n",
    "    # same query slightly slower\n",
    "    # ch0_tdc_active_scint = df_scint.filter(df_scint.CHAMBER==0).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch1_tdc_active_scint = df_scint.filter(df_scint.CHAMBER==1).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch2_tdc_active_scint = df_scint.filter(df_scint.CHAMBER==2).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch3_tdc_active_scint = df_scint.filter(df_scint.CHAMBER==3).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "\n",
    "    ch0_drift_times = df_scint.filter(df_scint.CHAMBER==0).select(\"DRIFT_TIME\")\n",
    "    ch1_drift_times = df_scint.filter(df_scint.CHAMBER==1).select(\"DRIFT_TIME\")\n",
    "    ch2_drift_times = df_scint.filter(df_scint.CHAMBER==2).select(\"DRIFT_TIME\")\n",
    "    ch3_drift_times = df_scint.filter(df_scint.CHAMBER==3).select(\"DRIFT_TIME\")\n",
    "\n",
    "    # get results (trigger actions on dataframes) and compute histograms\n",
    "\n",
    "    hit_count_chamber = hit_count_chamber.toPandas().values.reshape(-1)\n",
    "\n",
    "    ch0_tdc_counts_hist, ch0_tdc_counts_be = np.histogram(ch0_tdc_counts.toPandas().values.reshape(-1), bins = edges_list)\n",
    "    ch1_tdc_counts_hist, ch1_tdc_counts_be = np.histogram(ch1_tdc_counts.toPandas().values.reshape(-1), bins = edges_list)\n",
    "    ch2_tdc_counts_hist, ch2_tdc_counts_be = np.histogram(ch2_tdc_counts.toPandas().values.reshape(-1), bins = edges_list)\n",
    "    ch3_tdc_counts_hist, ch3_tdc_counts_be = np.histogram(ch3_tdc_counts.toPandas().values.reshape(-1), bins = edges_list)\n",
    "\n",
    "    ch0_tdc_active_hist, ch0_tdc_active_be = np.histogram(ch0_tdc_active.toPandas().values.reshape(-1), bins=edge_list_b)\n",
    "    ch1_tdc_active_hist, ch1_tdc_active_be = np.histogram(ch1_tdc_active.toPandas().values.reshape(-1), bins=edge_list_b)\n",
    "    ch2_tdc_active_hist, ch2_tdc_active_be = np.histogram(ch2_tdc_active.toPandas().values.reshape(-1), bins=edge_list_b)\n",
    "    ch3_tdc_active_hist, ch3_tdc_active_be = np.histogram(ch3_tdc_active.toPandas().values.reshape(-1), bins=edge_list_b)\n",
    "\n",
    "    ch0_tdc_active_scint_hist, ch0_tdc_active_scint_be = np.histogram(ch0_tdc_active_scint.toPandas().values.reshape(-1), bins=edge_list_b)\n",
    "    ch1_tdc_active_scint_hist, ch1_tdc_active_scint_be = np.histogram(ch1_tdc_active_scint.toPandas().values.reshape(-1), bins=edge_list_b)\n",
    "    ch2_tdc_active_scint_hist, ch2_tdc_active_scint_be = np.histogram(ch2_tdc_active_scint.toPandas().values.reshape(-1), bins=edge_list_b)\n",
    "    ch3_tdc_active_scint_hist, ch3_tdc_active_scint_be = np.histogram(ch3_tdc_active_scint.toPandas().values.reshape(-1), bins=edge_list_b)\n",
    "\n",
    "    ch0_drift_times_hist, ch0_drift_times_be = np.histogram(ch0_drift_times.toPandas().values.reshape(-1), bins=edge_list_c)\n",
    "    ch1_drift_times_hist, ch1_drift_times_be = np.histogram(ch1_drift_times.toPandas().values.reshape(-1), bins=edge_list_c)\n",
    "    ch2_drift_times_hist, ch2_drift_times_be = np.histogram(ch2_drift_times.toPandas().values.reshape(-1), bins=edge_list_c)\n",
    "    ch3_drift_times_hist, ch3_drift_times_be = np.histogram(ch3_drift_times.toPandas().values.reshape(-1), bins=edge_list_c)\n",
    "\n",
    "    df.unpersist()\n",
    "    df_scint.unpersist()\n",
    "    tdc_counts.unpersist()\n",
    "    tdc_active.unpersist()\n",
    "    tdc_active_scint.unpersist()\n",
    "    \n",
    "    \n",
    "    global ID\n",
    "    ID += 1\n",
    "\n",
    "    # prepare message to send to kafka\n",
    "\n",
    "    msg = {\n",
    "        'msg_ID': ID,\n",
    "        'hit_count': hit_count,\n",
    "        'hit_count_chamber': hit_count_chamber.tolist(),\n",
    "        'tdc_counts_chamber': {\n",
    "            '0': {\n",
    "                'bin_edges': edges_list_to_print,\n",
    "                'hist_counts': ch0_tdc_counts_hist.tolist()\n",
    "            },\n",
    "            '1': {\n",
    "                'bin_edges': edges_list_to_print,\n",
    "                'hist_counts': ch1_tdc_counts_hist.tolist()\n",
    "            },\n",
    "            '2': {\n",
    "                'bin_edges': edges_list_to_print,\n",
    "                'hist_counts': ch2_tdc_counts_hist.tolist()\n",
    "            },\n",
    "            '3': {\n",
    "                'bin_edges': edges_list_to_print,\n",
    "                'hist_counts': ch3_tdc_counts_hist.tolist()\n",
    "            }\n",
    "        },\n",
    "        'active_tdc_chamber': {\n",
    "            '0': {\n",
    "                'bin_edges': edge_list_b_to_print,\n",
    "                'hist_counts': ch0_tdc_active_hist.tolist()\n",
    "            },\n",
    "            '1': {\n",
    "                'bin_edges': edge_list_b_to_print,\n",
    "                'hist_counts': ch1_tdc_active_hist.tolist()\n",
    "            },\n",
    "            '2': {\n",
    "                'bin_edges': edge_list_b_to_print,\n",
    "                'hist_counts': ch2_tdc_active_hist.tolist()\n",
    "            },\n",
    "            '3': {\n",
    "                'bin_edges': edge_list_b_to_print,\n",
    "                'hist_counts': ch3_tdc_active_hist.tolist()\n",
    "            }\n",
    "        },\n",
    "        'active_tdc_chamber_scint': {\n",
    "            '0': {\n",
    "                'bin_edges': edge_list_b_to_print,\n",
    "                'hist_counts': ch0_tdc_active_scint_hist.tolist()\n",
    "            },\n",
    "            '1': {\n",
    "                'bin_edges': edge_list_b_to_print,\n",
    "                'hist_counts': ch1_tdc_active_scint_hist.tolist()\n",
    "            },\n",
    "            '2': {\n",
    "                'bin_edges': edge_list_b_to_print,\n",
    "                'hist_counts': ch2_tdc_active_scint_hist.tolist()\n",
    "            },\n",
    "            '3': {\n",
    "                'bin_edges': edge_list_b_to_print,\n",
    "                'hist_counts': ch3_tdc_active_scint_hist.tolist()\n",
    "            }\n",
    "        },\n",
    "        'drift_times': {\n",
    "            '0': {\n",
    "                'bin_edges': edge_list_c_to_print,\n",
    "                'hist_counts': ch0_drift_times_hist.tolist()\n",
    "            },\n",
    "            '1': {\n",
    "                'bin_edges': edge_list_c_to_print,\n",
    "                'hist_counts': ch1_drift_times_hist.tolist()\n",
    "            },\n",
    "            '2': {\n",
    "                'bin_edges': edge_list_c_to_print,\n",
    "                'hist_counts': ch2_drift_times_hist.tolist()\n",
    "            },\n",
    "            '3': {\n",
    "                'bin_edges': edge_list_c_to_print,\n",
    "                'hist_counts': ch3_drift_times_hist.tolist()\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    producer.produce(TOPIC_NAME, json.dumps(msg).encode('utf-8'))\n",
    "    producer.poll(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b056f17",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stop here, start the consumer (Kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c7a99",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.writeStream\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .foreachBatch(batch_func)\\\n",
    "    .trigger(processingTime='5 seconds')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}