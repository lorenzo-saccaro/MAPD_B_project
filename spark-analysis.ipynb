{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ADMIN SECTION: create and delete topics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS = '10.67.22.61:9092'\n",
    "TOPIC_NAME = 'results'\n",
    "N_PARTITIONS = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from  confluent_kafka.admin import AdminClient, NewTopic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "kafka_admin = AdminClient({'bootstrap.servers':KAFKA_BOOTSTRAP_SERVERS})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_topics(admin, topics):\n",
    "    \"\"\" Create topics \"\"\"\n",
    "\n",
    "    new_topics = [NewTopic(topic, num_partitions=N_PARTITIONS, replication_factor=1) for topic in topics]\n",
    "    # Call create_topics to asynchronously create topics, a dict\n",
    "    # of <topic,future> is returned.\n",
    "    fs = admin.create_topics(new_topics, request_timeout=15.0)\n",
    "\n",
    "    # Wait for operation to finish.\n",
    "    # Timeouts are preferably controlled by passing request_timeout=15.0\n",
    "    # to the create_topics() call.\n",
    "    # All futures will finish at the same time.\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()  # The result itself is None\n",
    "            print(\"Topic {} created\".format(topic))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create topic {}: {}\".format(topic, e))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def delete_topics(admin, topics):\n",
    "    \"\"\" delete topics \"\"\"\n",
    "\n",
    "    # Call delete_topics to asynchronously delete topics, a future is returned.\n",
    "    # By default, this operation on the broker returns immediately while\n",
    "    # topics are deleted in the background. But here we give it some time (30s)\n",
    "    # to propagate in the cluster before returning.\n",
    "    #\n",
    "    # Returns a dict of <topic,future>.\n",
    "    fs = admin.delete_topics(topics, operation_timeout=30)\n",
    "\n",
    "    # Wait for operation to finish.\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()  # The result itself is None\n",
    "            print(\"Topic {} deleted\".format(topic))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to delete topic {}: {}\".format(topic, e))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic results deleted\n"
     ]
    }
   ],
   "source": [
    "delete_topics(kafka_admin, [TOPIC_NAME])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic results created\n"
     ]
    }
   ],
   "source": [
    "## check if topic already exits otherwise create it\n",
    "if not TOPIC_NAME in kafka_admin.list_topics().topics.keys():\n",
    "    create_topics(kafka_admin, [TOPIC_NAME])\n",
    "else:\n",
    "    print(\"Topic \" + TOPIC_NAME + \" already exists\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create producer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "producer = Producer({'bootstrap.servers':KAFKA_BOOTSTRAP_SERVERS,\n",
    "                     'linger.ms':0, # delay in ms before messages are sent if batch size is not reached\n",
    "                     'batch.size':16384}) # maximum batch size before messages are sent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SPARK STRUCTURED STREAMING ANALYSIS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, countDistinct, count\n",
    "import numpy as np\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e5b20fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://10.67.22.29:7077\")\\\n",
    "        .appName(\"Test streaming\")\\\n",
    "        .config('spark.executor.memory', '4g')\\\n",
    "        .config('spark.driver.memory', '1500m')\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "        # .config('spark.executor.instances', '4')\\\n",
    "        # .config('spark.executor.cores', '1')\\\n",
    "        # .config('spark.executor.memory', '1g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# this may be the only option that matters (set it equal to the number of cores)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "# spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.cbo.enabled\", \"true\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<SparkContext master=spark://10.67.22.29:7077 appName=Test streaming>",
      "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://master:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.67.22.29:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Test streaming</code></dd>\n            </dl>\n        </div>\n        "
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# read streaming df from kafka\n",
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 4000)\\\n",
    "    .option('subscribe', 'data')\\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDF.isStreaming"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDF.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# extract the value from the kafka message\n",
    "csv_df = inputDF.select(col(\"value\").cast(\"string\")).alias(\"csv\").select(\"csv.*\")\n",
    "\n",
    "# split the csv line in the corresponding fields\n",
    "df = csv_df.selectExpr(\"cast(split(value, ',')[0] as int) as HEAD\",\n",
    "                       \"cast(split(value, ',')[1] as int) as FPGA\",\n",
    "                       \"cast(split(value, ',')[2] as int) as TDC_CHANNEL\",\n",
    "                       \"cast(split(value, ',')[3] as long) as ORBIT_CNT\",\n",
    "                       \"cast(split(value, ',')[4] as int) as BX_COUNTER\",\n",
    "                       \"cast(split(value, ',')[5] as double) as TDC_MEAS\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: long (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# remove unwanted rows\n",
    "df = df.filter(df.HEAD==2)\n",
    "\n",
    "# add CHAMBER column for easier grouping later\n",
    "df = df.withColumn(\"CHAMBER\", \\\n",
    "                    when((df.FPGA == 0)&(df.TDC_CHANNEL>=0)&(df.TDC_CHANNEL<64), 0) \\\n",
    "                   .when((df.FPGA == 0)&(df.TDC_CHANNEL>=64)&(df.TDC_CHANNEL<128), 1 ) \\\n",
    "                   .when((df.FPGA == 1)&(df.TDC_CHANNEL>=0)&(df.TDC_CHANNEL<64), 2 ) \\\n",
    "                   .when((df.FPGA == 1)&(df.TDC_CHANNEL>=64)&(df.TDC_CHANNEL<128), 3 )\n",
    "                  )\n",
    "\n",
    "# compute absolute time\n",
    "df = df.withColumn(\"ABSOLUTE_TIME\", 25*(df.ORBIT_CNT * 3564 + df.BX_COUNTER + df.TDC_MEAS/30))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: long (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      " |-- CHAMBER: integer (nullable = true)\n",
      " |-- ABSOLUTE_TIME: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "time_offset_by_chamber = {0: 95.0 - 1.1, # Ch 0\n",
    "                          1: 95.0 + 6.4, # Ch 1\n",
    "                          2: 95.0 + 0.5, # Ch 2\n",
    "                          3: 95.0 - 2.6, # Ch 3\n",
    "                         }\n",
    "\n",
    "# function to apply to each batch: writes and send a kafka message at the end\n",
    "def batch_func(df, epoch_id):\n",
    "\n",
    "    df.persist()\n",
    "\n",
    "    # 1: total number of processed hits, post-cleansing (1 value per batch)\n",
    "\n",
    "    hit_count = df.count()\n",
    "\n",
    "    # 2: total number of processed hits, post-cleansing, per chamber (4 values per batch)\n",
    "\n",
    "    hit_count_chamber = df.groupby('CHAMBER').agg(count('TDC_CHANNEL').alias('HIT_COUNT')).sort(\"CHAMBER\").select('HIT_COUNT')\n",
    "\n",
    "    # 3: histogram of the counts of active TDC_CHANNEL, per chamber (4 arrays per batch)\n",
    "\n",
    "    tdc_counts = df.groupby(['CHAMBER', 'TDC_CHANNEL']).agg(count('ORBIT_CNT').alias('TDC_COUNTS'))\n",
    "    tdc_counts.persist()\n",
    "\n",
    "    ch0_tdc_counts = tdc_counts.filter(tdc_counts.CHAMBER==0).select('TDC_COUNTS')\n",
    "    ch1_tdc_counts = tdc_counts.filter(tdc_counts.CHAMBER==1).select('TDC_COUNTS')\n",
    "    ch2_tdc_counts = tdc_counts.filter(tdc_counts.CHAMBER==2).select('TDC_COUNTS')\n",
    "    ch3_tdc_counts = tdc_counts.filter(tdc_counts.CHAMBER==3).select('TDC_COUNTS')\n",
    "\n",
    "    # same query slightly slower\n",
    "    # ch0_tdc_counts = df.filter(df.CHAMBER==0).groupby('TDC_CHANNEL').agg(count('ORBIT_CNT').alias('TDC_COUNTS')).select('TDC_COUNTS')\n",
    "    # ch1_tdc_counts = df.filter(df.CHAMBER==1).groupby('TDC_CHANNEL').agg(count('ORBIT_CNT').alias('TDC_COUNTS')).select('TDC_COUNTS')\n",
    "    # ch2_tdc_counts = df.filter(df.CHAMBER==2).groupby('TDC_CHANNEL').agg(count('ORBIT_CNT').alias('TDC_COUNTS')).select('TDC_COUNTS')\n",
    "    # ch3_tdc_counts = df.filter(df.CHAMBER==3).groupby('TDC_CHANNEL').agg(count('ORBIT_CNT').alias('TDC_COUNTS')).select('TDC_COUNTS')\n",
    "\n",
    "    # 4: histogram of the total number of active TDC_CHANNEL in each ORBIT_CNT, per chamber (4 arrays per batch)\n",
    "\n",
    "    tdc_active = df.groupby(['CHAMBER', 'ORBIT_CNT']).agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\"))\n",
    "    tdc_active.persist()\n",
    "\n",
    "    ch0_tdc_active = tdc_active.filter(tdc_active.CHAMBER==0).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch1_tdc_active = tdc_active.filter(tdc_active.CHAMBER==1).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch2_tdc_active = tdc_active.filter(tdc_active.CHAMBER==2).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch3_tdc_active = tdc_active.filter(tdc_active.CHAMBER==3).select('ACTIVE_TDC_CHANNEL')\n",
    "\n",
    "    # same query slightly slower\n",
    "    # ch0_tdc_active = df.filter(df.CHAMBER==0).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch1_tdc_active = df.filter(df.CHAMBER==1).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch2_tdc_active = df.filter(df.CHAMBER==2).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch3_tdc_active = df.filter(df.CHAMBER==3).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "\n",
    "    # optional: drift time computation\n",
    "\n",
    "    # get scintillator signals\n",
    "    t0 = df.filter((df.FPGA==1) & (df.TDC_CHANNEL==128)).selectExpr('ORBIT_CNT as ORBIT_CNT_T0', 'ABSOLUTE_TIME as ABSOLUTE_TIME_T0')\n",
    "    # TODO: add check for multiple signal in same orbit\n",
    "\n",
    "    # get only events with a signal from the scintillator and compute drift time\n",
    "    df_scint = df.join(t0, df.ORBIT_CNT==t0.ORBIT_CNT_T0, 'inner').selectExpr('CHAMBER', 'TDC_CHANNEL', 'ORBIT_CNT', 'ABSOLUTE_TIME-ABSOLUTE_TIME_T0 as DRIFT_TIME')\n",
    "\n",
    "    # adjust drift time with the correct\n",
    "    df_scint = df_scint.withColumn('DRIFT_TIME',\n",
    "                       when(df_scint.CHAMBER==0, df_scint.DRIFT_TIME+time_offset_by_chamber[0]) \\\n",
    "                      .when(df_scint.CHAMBER==1, df_scint.DRIFT_TIME+time_offset_by_chamber[1]) \\\n",
    "                      .when(df_scint.CHAMBER==2, df_scint.DRIFT_TIME+time_offset_by_chamber[2]) \\\n",
    "                      .when(df_scint.CHAMBER==3, df_scint.DRIFT_TIME+time_offset_by_chamber[3])\n",
    "                  )\n",
    "\n",
    "    df_scint.persist()\n",
    "\n",
    "    tdc_active_scint = df_scint.groupby(['CHAMBER', 'ORBIT_CNT']).agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\"))\n",
    "    tdc_active_scint.persist()\n",
    "\n",
    "    ch0_tdc_active_scint = tdc_active_scint.filter(tdc_active_scint.CHAMBER==0).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch1_tdc_active_scint = tdc_active_scint.filter(tdc_active_scint.CHAMBER==1).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch2_tdc_active_scint = tdc_active_scint.filter(tdc_active_scint.CHAMBER==2).select('ACTIVE_TDC_CHANNEL')\n",
    "    ch3_tdc_active_scint = tdc_active_scint.filter(tdc_active_scint.CHAMBER==3).select('ACTIVE_TDC_CHANNEL')\n",
    "\n",
    "    # same query slightly slower\n",
    "    # ch0_tdc_active_scint = df_scint.filter(df_scint.CHAMBER==0).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch1_tdc_active_scint = df_scint.filter(df_scint.CHAMBER==1).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch2_tdc_active_scint = df_scint.filter(df_scint.CHAMBER==2).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "    # ch3_tdc_active_scint = df_scint.filter(df_scint.CHAMBER==3).groupby('ORBIT_CNT').agg(countDistinct(\"TDC_CHANNEL\").alias(\"ACTIVE_TDC_CHANNEL\")).select('ACTIVE_TDC_CHANNEL')\n",
    "\n",
    "    ch0_drift_times = df_scint.filter(df_scint.CHAMBER==0).select(\"DRIFT_TIME\")\n",
    "    ch1_drift_times = df_scint.filter(df_scint.CHAMBER==1).select(\"DRIFT_TIME\")\n",
    "    ch2_drift_times = df_scint.filter(df_scint.CHAMBER==2).select(\"DRIFT_TIME\")\n",
    "    ch3_drift_times = df_scint.filter(df_scint.CHAMBER==3).select(\"DRIFT_TIME\")\n",
    "\n",
    "    # get results (trigger actions on dataframes) and compute histograms\n",
    "\n",
    "    hit_count_chamber = hit_count_chamber.toPandas().values.reshape(-1)\n",
    "\n",
    "    ch0_tdc_counts_hist, ch0_tdc_counts_be = np.histogram(ch0_tdc_counts.toPandas().values.reshape(-1), bins=10)\n",
    "    ch1_tdc_counts_hist, ch1_tdc_counts_be = np.histogram(ch1_tdc_counts.toPandas().values.reshape(-1), bins=10)\n",
    "    ch2_tdc_counts_hist, ch2_tdc_counts_be = np.histogram(ch2_tdc_counts.toPandas().values.reshape(-1), bins=10)\n",
    "    ch3_tdc_counts_hist, ch3_tdc_counts_be = np.histogram(ch3_tdc_counts.toPandas().values.reshape(-1), bins=10)\n",
    "\n",
    "    ch0_tdc_active_hist, ch0_tdc_active_be = np.histogram(ch0_tdc_active.toPandas().values.reshape(-1), bins=10)\n",
    "    ch1_tdc_active_hist, ch1_tdc_active_be = np.histogram(ch1_tdc_active.toPandas().values.reshape(-1), bins=10)\n",
    "    ch2_tdc_active_hist, ch2_tdc_active_be = np.histogram(ch2_tdc_active.toPandas().values.reshape(-1), bins=10)\n",
    "    ch3_tdc_active_hist, ch3_tdc_active_be = np.histogram(ch3_tdc_active.toPandas().values.reshape(-1), bins=10)\n",
    "\n",
    "    ch0_tdc_active_scint_hist, ch0_tdc_active_scint_be = np.histogram(ch0_tdc_active_scint.toPandas().values.reshape(-1), bins=10)\n",
    "    ch1_tdc_active_scint_hist, ch1_tdc_active_scint_be = np.histogram(ch1_tdc_active_scint.toPandas().values.reshape(-1), bins=10)\n",
    "    ch2_tdc_active_scint_hist, ch2_tdc_active_scint_be = np.histogram(ch2_tdc_active_scint.toPandas().values.reshape(-1), bins=10)\n",
    "    ch3_tdc_active_scint_hist, ch3_tdc_active_scint_be = np.histogram(ch3_tdc_active_scint.toPandas().values.reshape(-1), bins=10)\n",
    "\n",
    "    ch0_drift_times_hist, ch0_drift_times_be = np.histogram(ch0_drift_times.toPandas().values.reshape(-1), bins=10)\n",
    "    ch1_drift_times_hist, ch1_drift_times_be = np.histogram(ch1_drift_times.toPandas().values.reshape(-1), bins=10)\n",
    "    ch2_drift_times_hist, ch2_drift_times_be = np.histogram(ch2_drift_times.toPandas().values.reshape(-1), bins=10)\n",
    "    ch3_drift_times_hist, ch3_drift_times_be = np.histogram(ch3_drift_times.toPandas().values.reshape(-1), bins=10)\n",
    "\n",
    "    df.unpersist()\n",
    "    df_scint.unpersist()\n",
    "    tdc_counts.unpersist()\n",
    "    tdc_active.unpersist()\n",
    "    tdc_active_scint.unpersist()\n",
    "\n",
    "    # prepare message to send to kafka\n",
    "\n",
    "    msg = {\n",
    "        'hit_count': hit_count,\n",
    "        'hit_count_chamber': hit_count_chamber.tolist(),\n",
    "        'tdc_counts_chamber': {\n",
    "            '0': {\n",
    "                'bin_edges': ch0_tdc_counts_be.tolist(),\n",
    "                'hist_counts': ch0_tdc_counts_hist.tolist()\n",
    "            },\n",
    "            '1': {\n",
    "                'bin_edges': ch1_tdc_counts_be.tolist(),\n",
    "                'hist_counts': ch1_tdc_counts_hist.tolist()\n",
    "            },\n",
    "            '2': {\n",
    "                'bin_edges': ch2_tdc_counts_be.tolist(),\n",
    "                'hist_counts': ch2_tdc_counts_hist.tolist()\n",
    "            },\n",
    "            '3': {\n",
    "                'bin_edges': ch3_tdc_counts_be.tolist(),\n",
    "                'hist_counts': ch3_tdc_counts_hist.tolist()\n",
    "            }\n",
    "        },\n",
    "        'active_tdc_chamber': {\n",
    "            '0': {\n",
    "                'bin_edges': ch0_tdc_active_be.tolist(),\n",
    "                'hist_counts': ch0_tdc_active_hist.tolist()\n",
    "            },\n",
    "            '1': {\n",
    "                'bin_edges': ch1_tdc_active_be.tolist(),\n",
    "                'hist_counts': ch1_tdc_active_hist.tolist()\n",
    "            },\n",
    "            '2': {\n",
    "                'bin_edges': ch2_tdc_active_be.tolist(),\n",
    "                'hist_counts': ch2_tdc_active_hist.tolist()\n",
    "            },\n",
    "            '3': {\n",
    "                'bin_edges': ch3_tdc_active_be.tolist(),\n",
    "                'hist_counts': ch3_tdc_active_hist.tolist()\n",
    "            }\n",
    "        },\n",
    "        'active_tdc_chamber_scint': {\n",
    "            '0': {\n",
    "                'bin_edges': ch0_tdc_active_scint_be.tolist(),\n",
    "                'hist_counts': ch0_tdc_active_scint_hist.tolist()\n",
    "            },\n",
    "            '1': {\n",
    "                'bin_edges': ch1_tdc_active_scint_be.tolist(),\n",
    "                'hist_counts': ch1_tdc_active_scint_hist.tolist()\n",
    "            },\n",
    "            '2': {\n",
    "                'bin_edges': ch2_tdc_active_scint_be.tolist(),\n",
    "                'hist_counts': ch2_tdc_active_scint_hist.tolist()\n",
    "            },\n",
    "            '3': {\n",
    "                'bin_edges': ch3_tdc_active_scint_be.tolist(),\n",
    "                'hist_counts': ch3_tdc_active_scint_hist.tolist()\n",
    "            }\n",
    "        },\n",
    "        'drift_times': {\n",
    "            '0': {\n",
    "                'bin_edges': ch0_drift_times_be.tolist(),\n",
    "                'hist_counts': ch0_drift_times_hist.tolist()\n",
    "            },\n",
    "            '1': {\n",
    "                'bin_edges': ch1_drift_times_be.tolist(),\n",
    "                'hist_counts': ch1_drift_times_hist.tolist()\n",
    "            },\n",
    "            '2': {\n",
    "                'bin_edges': ch2_drift_times_be.tolist(),\n",
    "                'hist_counts': ch2_drift_times_hist.tolist()\n",
    "            },\n",
    "            '3': {\n",
    "                'bin_edges': ch3_drift_times_be.tolist(),\n",
    "                'hist_counts': ch3_drift_times_hist.tolist()\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    producer.produce(TOPIC_NAME, json.dumps(msg).encode('utf-8'))\n",
    "    producer.poll(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [23]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwriteStream\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutputMode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mupdate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforeachBatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_func\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrigger\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocessingTime\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m2 seconds\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/streaming.py:107\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1320\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1313\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1322\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.8/socket.py:669\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    668\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 669\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    671\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "df.writeStream\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .foreachBatch(batch_func)\\\n",
    "    .trigger(processingTime='2 seconds')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}