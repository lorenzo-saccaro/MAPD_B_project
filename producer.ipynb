{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## PARAMETERS ##\n",
    "\n",
    "N_PARTITIONS = 12\n",
    "BOOTSTRAP_SERVER = '10.67.22.61'\n",
    "MSG_RATE = 1000 # number of messages per second\n",
    "\n",
    "# parameters for artificial rate control\n",
    "BATCH_FRACTION = 0.1 # can't be lower than 0.1 this (problem with sleep function time resolution see https://stackoverflow.com/questions/1133857/how-accurate-is-pythons-time-sleep)\n",
    "BATCH_SIZE = int(max(0.1*MSG_RATE, BATCH_FRACTION*MSG_RATE)) # number of messages between each rate control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ADMIN SECTION: create and delete partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient, NewTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kafka_admin = AdminClient({'bootstrap.servers':BOOTSTRAP_SERVER})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_topics(admin, topics):\n",
    "    \"\"\" Create topics \"\"\"\n",
    "\n",
    "    new_topics = [NewTopic(topic, num_partitions=N_PARTITIONS, replication_factor=1) for topic in topics]\n",
    "    # Call create_topics to asynchronously create topics, a dict\n",
    "    # of <topic,future> is returned.\n",
    "    fs = admin.create_topics(new_topics, request_timeout=15.0)\n",
    "\n",
    "    # Wait for operation to finish.\n",
    "    # Timeouts are preferably controlled by passing request_timeout=15.0\n",
    "    # to the create_topics() call.\n",
    "    # All futures will finish at the same time.\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()  # The result itself is None\n",
    "            print(\"Topic {} created\".format(topic))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create topic {}: {}\".format(topic, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def delete_topics(admin, topics):\n",
    "    \"\"\" delete topics \"\"\"\n",
    "\n",
    "    # Call delete_topics to asynchronously delete topics, a future is returned.\n",
    "    # By default, this operation on the broker returns immediately while\n",
    "    # topics are deleted in the background. But here we give it some time (30s)\n",
    "    # to propagate in the cluster before returning.\n",
    "    #\n",
    "    # Returns a dict of <topic,future>.\n",
    "    fs = admin.delete_topics(topics, operation_timeout=30)\n",
    "\n",
    "    # Wait for operation to finish.\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()  # The result itself is None\n",
    "            print(\"Topic {} deleted\".format(topic))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to delete topic {}: {}\".format(topic, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic data deleted\n"
     ]
    }
   ],
   "source": [
    "delete_topics(kafka_admin, ['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create topic data: KafkaError{code=TOPIC_ALREADY_EXISTS,val=36,str=\"Topic 'data' is marked for deletion.\"}\n"
     ]
    }
   ],
   "source": [
    "## check if topic already exits otherwise create it\n",
    "topic_name = 'data'\n",
    "if not topic_name in kafka_admin.list_topics().topics.keys():\n",
    "    create_topics(kafka_admin, [topic_name])\n",
    "else:\n",
    "    print(\"Topic \" + topic_name + \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Connect to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3', endpoint_url='https://cloud-areapd.pd.infn.it:5210', verify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create producer and send messages at specified rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "producer = Producer({'bootstrap.servers':BOOTSTRAP_SERVER,\n",
    "                     'linger.ms':20, # delay in ms before messages are sent if batch size is not reached\n",
    "                     'batch.size':16384}) # maximum batch size before messages are sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stop here until spark is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'cloud-areapd.pd.infn.it'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: data_000000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'cloud-areapd.pd.infn.it'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: data_000001.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'cloud-areapd.pd.infn.it'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'mapd-minidt-stream'\n",
    "batch_count = 0 # counter for artificial delay\n",
    "\n",
    "# read all lines from all files in the bucket\n",
    "for key in s3_client.list_objects(Bucket=bucket_name)['Contents']:\n",
    "    print('file:', key[\"Key\"])\n",
    "    # create line iterator\n",
    "    line_reader = s3_client.get_object(Bucket=bucket_name, Key=key['Key'])['Body'].iter_lines()\n",
    "\n",
    "    next(line_reader) # skip header line for each file\n",
    "    \n",
    "    for line in line_reader:\n",
    "\n",
    "        producer.produce(topic_name, line) # produce message\n",
    "        producer.poll(0) # pool producer (asynch process, message are actually sent following criteria)\n",
    "        batch_count += 1 # update counter\n",
    "        if batch_count == BATCH_SIZE: # add artificial rate control\n",
    "            time.sleep(BATCH_SIZE/MSG_RATE)\n",
    "            batch_count = 0 # reset counter\n",
    "    \n",
    "    \n",
    "    \n",
    "producer.flush() # wait for last messages to be sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}